| Title| Year |Paper|
| ------- | ----- | ------ |
|[One Shot Learning as Instruction Data Prospector for Large Language Models](https://arxiv.org/abs/2312.10302)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2312.10302) ,[[Note]](https://mp.weixin.qq.com/s/YHGwdkvCN0Q1NSFsB61nYA)|
|[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288 )|Arxiv2023|[[Paper]](https://arxiv.org/abs/2307.09288 )|
|[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)|NeurIPS2023|[[Paper]](https://arxiv.org/abs/2305.14314)|
|[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2305.18290)|
|[WAVECODER: WIDESPREAD AND VERSATILE ENHANCED INSTRUCTION TUNING WITH REFINED DATA GENERATION](https://arxiv.org/pdf/2312.14187.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.14187.pdf)|
