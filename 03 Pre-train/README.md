| Name| Description |Resources|
| ------- | ----- | ------ |
|unilm|Large-scale Self-supervised Pre-training Across Tasks, Languages, and Modalities.|[[Code]](https://github.com/microsoft/unilm)|
|nanoGPT|The simplest, fastest repository for training/finetuning medium-sized GPTs.|[[Code]](https://github.com/karpathy/nanoGPT)|
|Megatron-LM|Ongoing research training transformer models at scale|[[Code]](https://github.com/NVIDIA/Megatron-LM)|
|Trl|Train transformer language models with reinforcement learning.|[[Code]](https://github.com/huggingface/trl)|



##### 其他资源

- [大模型训练loss突刺原因和解决办法](https://mp.weixin.qq.com/s/CYJLxux34V8mj35CH5Yfbw)
- [大模型训练中的 fp32/fp16/bf16、混合精度、训练溢出](https://mp.weixin.qq.com/s/bkZaPuU3jA1Shs46OvwlfA)
- [Attention机制汇总](https://mp.weixin.qq.com/s/t4ytOIuPx0799kkjFs5lbQ)
- [MoE讲解](https://mp.weixin.qq.com/s/I1D-mVQCseL4gW9sJLzY2w)
- [[Igniting Language Intelligence: The Hitchhiker’s Guide From Chain-of-Thought Reasoning to Language Agents]](https://arxiv.org/pdf/2311.11797.pdf) , [[Code]](https://github.com/Zoeyyao27/CoT-Igniting-Agent) , [[Note]](https://mp.weixin.qq.com/s/aYBrk3X-M32PdohLduCBNw)
- [数据并行](https://mp.weixin.qq.com/s/d6Q4fWfHPIJpMSXbPRijSA)

