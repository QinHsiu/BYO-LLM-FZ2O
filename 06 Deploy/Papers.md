<!--
 * @Author: qinxiuyuan
 * @Date: 2024-04-15 20:13:06
 * @LastEditTime: 2024-04-15 20:13:08
 * @FilePath: /SFT/qinxiuyuan/workplace/Learning/BYO-LLM-FZ2O/06 Deploy/Papers.md
 * @Description: 
 * @Email: qinhsiu@gmail.com
-->
| Title| Year |Resources|
| ------- | ----- | ------ |
|[Infinite-LLM: Efficient LLM Service for Long Context with DistAttention and Distributed KVCache](https://arxiv.org/pdf/2401.02669.pdf)|Arxiv2024|[[Paper]](https://arxiv.org/pdf/2401.02669.pdf) ,[[Note]](https://mp.weixin.qq.com/s/TAx3UEy10tlHphqccwQEVw)|
|[LLM-Pruner](https://arxiv.org/abs/2305.11627)|NeurIPS2023|[[Paper]](https://arxiv.org/abs/2305.11627) ,[[Code]](https://github.com/horseee/LLM-Pruner)|
|[GPTQ: Accurate Post-training Quantization of Generative Pretrained Transformers](https://arxiv.org/abs/2210.17323)|ICLR2023|[[Paper]](https://arxiv.org/abs/2210.17323) ,[[Code]](https://github.com/IST-DASLab/gptq) ,[[GPTQ-for-LLaMa]](https://github.com/qwopqwop200/GPTQ-for-LLaMa)|
|[Do Emergent Abilities Exist in Quantized Large Language Models: An Empirical Study](https://arxiv.org/abs/2307.08072)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2307.08072) ,[[Code]](https://github.com/RUCAIBox/QuantizedEmpirical)|
|[Octopus v2: On-device language model for super agent](https://arxiv.org/abs/2404.01744)|Arxiv2024|[[Codel]](https://huggingface.co/NexaAIDev/Octopus-v2),[[Note]](https://mp.weixin.qq.com/s/D65LBWMlOWRtNPc47ls89A)|
