|Type| Name|Title | Year  |   Resources   |
| ------- | -------  |------- | ----- | ------ | 
|事实性Factuality基准评测|BoolQ|[BoolQ: Exploring the surprising difficulty of natural yes/no questions](https://aclanthology.org/N19-1300/)|NAACL2019|[[Paper]](https://aclanthology.org/N19-1300/)|
|事实性Factuality基准评测|NaturalQuestions|[Natural questions: A benchmark for question answering research](https://aclanthology.org/Q19-1026)|NAACL2019|[[Paper]](https://aclanthology.org/Q19-1026)|
|事实性Factuality基准评测|RealtimeQA|[RealTime QA: What’s the answer right now?](https://arxiv.org/abs/2207.13332)|Arxiv2022|[[Paper]](https://arxiv.org/abs/2207.13332) ,[[Code]](https://github.com/realtimeqa/realtimeqa_public)|
|事实性Factuality基准评测|TydiQA-noContext|[TydiQA: A benchmark for information-seeking question answering in typo- logically diverse languages](https://storage.googleapis.com/tydiqa/tydiqa.pdf)|TACL2020|[[Paper]](https://storage.googleapis.com/tydiqa/tydiqa.pdf)|
|事实性Factuality基准评测|CSQA|[Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph](https://arxiv.org/abs/1801.10314)|AAAI2018|[[Paper]](https://arxiv.org/abs/1801.10314) ,[[Code]](https://github.com/iitm-nlp-miteshk/AmritaSaha/tree/master/CSQA)|
|Long Context|NarrativeQA|[The NarrativeQA reading comprehension challenge](https://aclanthology.org/Q18-1023)|TACL2018|[[Paper]](https://aclanthology.org/Q18-1023)|
|Long Context|Scrolls-Qasper|[SCROLLS: Standardized CompaRison over long language sequences](https://aclanthology.org/2022.emnlp-main.823)|EMNLP2022|[[Paper]](https://aclanthology.org/2022.emnlp-main.823) ,[[Code]](https://github.com/tau-nlp/scrolls)|
|Long Context|XLsum|[XL-sum: Large-scale multilingual abstractive summarization for 44 languages](https://aclanthology.org/2021.findings-acl.413)|ACL-IJCNLP2021|[[Paper]](https://aclanthology.org/2021.findings-acl.413)|
|Math/Science|GSM8k|[Training verifiers to solve math word problems](https://arxiv.org/abs/2110.14168)|Arxiv2021|[[Paper]](https://arxiv.org/abs/2110.14168) ,[[code]](https://github.com/openai/grade-school-math)|
|Math/Science|Math|[Measuring mathematical problem solving with the MATH dataset](https://arxiv.org/abs/2103.03874)|Arxiv2021|[[Paper]](https://arxiv.org/abs/2103.03874)|
|Math/Science|MMLU|[Measuring massive multitask language understanding](https://arxiv.org/abs/2009.03300)|ICLR2021|[[Paper]](https://arxiv.org/abs/2009.03300) ,[[Code]](https://github.com/hendrycks/test)|
|Math/Science|Math-StackExchange|[Automatic generation of headlines for online math questions](https://arxiv.org/abs/1912.00839)|AAAI2020|[[Paper]](https://arxiv.org/abs/1912.00839) ,[[Code]](https://github.com/yuankepku/MathSum)|
|Math/Science| GaoKao-Benchmark|[Evaluating the Performance of Large Language Models on GAOKAO Benchmark](https://arxiv.org/pdf/2305.12474.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2305.12474.pdf) ,[[Code]](https://github.com/OpenLMLab/GAOKAO-Bench)|
|Math/Science|C-eval|[C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models](https://arxiv.org/pdf/2305.08322.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2305.08322.pdf) ,[[Code]](https://github.com/hkust-nlp/ceval)|
|Reasoning|BigBench|[Beyond the imitation game: Quantifying and extrapolating the capabilities of language models](https://arxiv.org/pdf/2206.04615.pdf)|Arxiv2022|[[Paper]](https://arxiv.org/pdf/2206.04615.pdf) ,[[Code]](https://github.com/google/BIG-bench)|
|Reasoning|CLRS|[The clrs algorithmic reasoning benchmark](https://arxiv.org/abs/2205.15659v1)|ICML2022|[[Paper]](https://arxiv.org/abs/2205.15659v1) ,[[Code]](https://github.com/google-deepmind/clrs)|
|Reasoning|Proof Writer|[Proof Writer: Generating implications, proofs, and abductive statements over natural language](https://arxiv.org/pdf/2012.13048.pdf)|ACl2021|[[Paper]](https://arxiv.org/pdf/2012.13048.pdf) ,[Code](https://github.com/google-research/text-to-text-transfer-transformer)|
|Reasoning|Reasoning-Fermi problems|[How much coffee was consumed during emnlp 2019? fermi problems: A new reasoning challenge for ai](https://arxiv.org/abs/2110.14207)|EMNLP2021|[[Paper]](https://arxiv.org/abs/2110.14207)|
|Reasoning|Lambada|[The LAMBADA dataset: Word prediction requiring a broad discourse context](https://arxiv.org/abs/1606.06031)|ACL2016|[[Paper]](https://arxiv.org/abs/1606.06031)|
|Reasoning|HellaSwag|[Hellaswag: Can a machine really finish your sentence?](https://arxiv.org/abs/1905.07830)|ACL2019|[[Paper]](https://arxiv.org/abs/1905.07830)|
|Reasoning|DROP|[DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs](https://aclanthology.org/N19-1246)|NAACL2019|[[Paper]](https://aclanthology.org/N19-1246) ,[[Code]](https://github.com/huggingface/transformers)|
|Reasoning|PIQA|[Piqa: Reasoning about physical commonsense in natural language](https://arxiv.org/abs/1911.11641)|AAAI2020|[[Paper]](https://arxiv.org/abs/1911.11641)|
|Summarization|XL Sum|[XL-sum: Large-scale multilingual abstractive summarization for 44 languages](https://aclanthology.org/2021.findings-acl.413)|ACL-IJCNLP2021|[[Paper]](https://aclanthology.org/2021.findings-acl.413) ,[[Code]](https://github.com/csebuetnlp/xl-sum)|
|Summarization|WikiLingua|[WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization](https://aclanthology.org/2020.findings-emnlp.360/)|EMNLP2020|[[Paper]](https://aclanthology.org/2020.findings-emnlp.360/) ,[[Code]](https://github.com/esdurmus/Wikilingua)|
|Summarization|XSum|[Don’t give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization](https://aclanthology.org/D18-1206)|EMNLP2018|[[Paper]](https://aclanthology.org/D18-1206) ,[[Code]](https://github.com/EdinburghNLP/XSum)|
|Multilinguality|XLSum|[XL-sum: Large-scale multilingual abstractive summarization for 44 languages](https://aclanthology.org/2021.findings-acl.413)|ACL-IJCNLP2021|[[Paper]](https://aclanthology.org/2021.findings-acl.413) ,[[Code]](https://github.com/csebuetnlp/xl-sum)|
|Multilinguality|WMT22|[Findings of the 2022 Conference on Machine Translation (WMT22)](https://aclanthology.org/2022.wmt-1.1)|WMT2022|[[Paper]](https://aclanthology.org/2022.wmt-1.1) ,[[Code]](https://www.statmt.org/wmt22/)|
|Multilinguality|WMT23|[Findings of the 2023 conference on machine translation (wmt23): Llms are here but not quite there yet](https://aclanthology.org/2023.wmt-1.1/)|WMT2023|[[Paper]](https://aclanthology.org/2023.wmt-1.1/) ,[[Code]](https://github.com/wmt-conference/wmt23-news-systems)|
|Multilinguality|FRMT|[Frmt: A benchmark for few-shot region-aware machine translation](https://arxiv.org/abs/2210.00193)|TACL2023|[[Paper]](https://arxiv.org/abs/2210.00193) ,[[Code]](https://github.com/google-research/google-research/tree/master/frmt)|
|Multilinguality|WikiLingua|[WikiLingua: A new benchmark dataset for cross-lingual abstractive summarization](https://www.aclweb.org/anthology/2020.findings-emnlp.360)|EMNLP2020|[[Paper]](https://www.aclweb.org/anthology/2020.findings-emnlp.360)|
|Multilinguality|TydiQA|[TydiQA: A benchmark for information-seeking question answering in typo- logically diverse languages](https://storage.googleapis.com/tydiqa/tydiqa.pdf)|TACL2020|[[Paper]](https://storage.googleapis.com/tydiqa/tydiqa.pdf)|
|Multilinguality|MGSM|[Language models are multilingual chain-of- thought reasoners](https://arxiv.org/abs/2210.03057)|ICLR2023|[[Paper]](https://arxiv.org/abs/2210.03057)|
|Multilinguality|MMLU|[Measuring massive multitask language understanding](https://arxiv.org/abs/2009.03300)|ICLR2021|[[Paper]](https://arxiv.org/abs/2009.03300)|
|Multilinguality|NTREX|[NTREX-128 – news test references for MT evaluation of 128 languages](https://aclanthology.org/2022.sumeval-1.4)|ACL2022|[[Paper]](https://aclanthology.org/2022.sumeval-1.4)|
|Multilinguality|FLORES-200|[No language left behind: Scaling human-centered machine translation](https://arxiv.org/abs/2207.04672)|Arxiv2022|[[Paper]](https://arxiv.org/abs/2207.04672) ,[[Code]](https://github.com/facebookresearch/fairseq/tree/nllb)|
|Image Understanding|MMMU|[Mmmu: A massive multi-discipline multimodal understanding and reasoning benchmark for expert agi](https://arxiv.org/abs/2311.16502)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2311.16502) ,[[Code]](https://github.com/MMMU-Benchmark/MMMU)|
|Image Understanding|TextVQA|[Towards VQA models that can read](https://arxiv.org/pdf/1904.08920.pdf)|CVPR2019|[[Paper]](https://arxiv.org/pdf/1904.08920.pdf) ,[[Code]](https://github.com/facebookresearch/mmf)|
|Image Understanding|DocVQA|[Docvqa: A dataset for vqa on document images](https://arxiv.org/abs/2007.00398)|WACV2021|[[Paper]](https://arxiv.org/abs/2007.00398) ,[[Code]](https://www.docvqa.org/)|
|Image Understanding|ChartQA|[ChartQA: A benchmark for question answering about charts with visual and logical reasoning](https://arxiv.org/abs/2203.10244)|ACL2022|[[Paper]](https://arxiv.org/abs/2203.10244)|
|Image Understanding|InfographicVQA|[InfographicVQA](https://arxiv.org/pdf/2104.12756.pdf)|WACV2022|[[Paper]](https://arxiv.org/pdf/2104.12756.pdf)|
|Image Understanding|MathVista|[Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts](https://arxiv.org/abs/2310.02255)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2310.02255) ,[[Code]](https://mathvista.github.io/)|
|Image Understanding|AI2D|[A diagram is worth a dozen images](https://arxiv.org/abs/1603.07396)|ECCV2016|[[Paper]](https://arxiv.org/abs/1603.07396)|
|Image Understanding|VQAv2|[Making the V in VQA matter: Elevating the role of image understanding in visual question answering](https://arxiv.org/pdf/1612.00837.pdf)|ICCV2017|[[Paper]](https://arxiv.org/pdf/1612.00837.pdf)|
|Image Understanding|XM3600|[Crossmodal-3600: A massively multilingual multimodal evaluation dataset](https://arxiv.org/pdf/2205.12522.pdf)|EMNLP2022|[[Paper]](https://arxiv.org/pdf/2205.12522.pdf) ,[[Code]](https://google.github.io/crossmodal-3600/)|
|Image Understanding|Memecap|[MemeCap: A Dataset for Captioning and Interpreting Memes](https://arxiv.org/abs/2305.13703)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2305.13703)|
|Video Understanding|VATEX|[VATEX: A large-scale, high-quality multilingual dataset for video-and-language research](https://arxiv.org/abs/1904.03493)|ICCV2019|[[Paper]](https://arxiv.org/abs/1904.03493)|
|Video Understanding|YouCook2|[Towards automatic learning of procedures from web instructional videos](https://arxiv.org/abs/1703.09788)|AAI2018|[[Paper]](https://arxiv.org/abs/1703.09788)|
|Video Understanding|NextQA|[NExT-QA: Next phase of question-answering to explaining temporal actions](https://arxiv.org/abs/2105.08276)|CVPR2021|[[Paper]](https://arxiv.org/abs/2105.08276) ,[[Code]](https://github.com/doc-doc/NExT-QA)|
|Video Understanding|ActivityNext-QA|[ ActivityNet-QA: A dataset for understanding complex web videos via question answering](https://arxiv.org/abs/1906.02467)|AAAI2019|[[Paper]](https://arxiv.org/abs/1906.02467)|
|Video Understanding|Perception Text MCQA|[Perception test: A diagnostic benchmark for multimodal video models](https://arxiv.org/abs/2305.13786)|NeurIPS2023|[[Paper]](https://arxiv.org/abs/2305.13786)|
|Audio|FLEURS|[Fleurs: Few-shot learning evaluation of universal representations of speech](https://arxiv.org/abs/2205.12446)|SLT2023|[[Paper]](https://arxiv.org/abs/2205.12446)|
|Audio|VoxPopul|[Voxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation](https://arxiv.org/pdf/2101.00390.pdf)|ACL2021|[[Paper]](https://arxiv.org/pdf/2101.00390.pdf)|
|Audio|Librispeech|[Librispeech: an asr corpus based on public domain audio books](https://paperswithcode.com/dataset/librispeech)|ICASSP2015|[[Paper]](https://paperswithcode.com/dataset/librispeech)|
|Audio|CoVoST2|[Covost 2 and massively multilingual speech-to-text translation](https://arxiv.org/abs/2007.10310)|Aarxiv2020|[[Paper]](https://arxiv.org/abs/2007.10310)|
|Coding|MBPP|[Program Synthesis with Large Language Models](https://arxiv.org/abs/2108.07732)|Arxiv2021|[[Paper]](https://arxiv.org/abs/2108.07732)|
|Coding|CodexGLUE|[Codexglue: A machine learning benchmark dataset for code understanding and generation](https://arxiv.org/abs/2102.04664)|Arxiv2021|[[Paper]](https://arxiv.org/abs/2102.04664)|
|Coding|HumanEval|[Evaluating large language models trained on code](https://arxiv.org/abs/2107.03374)|Arxiv2021|[[Paper]](https://arxiv.org/abs/2107.03374)|
|Coding|CodeContests|[Competition-level code generation with alphacode](https://arxiv.org/abs/2203.07814)|Arxiv2022|[[Paper]](https://arxiv.org/abs/2203.07814)|
|Coding|APPS|[Measuring coding challenge competence with apps](https://arxiv.org/abs/2105.09938)|NeurIPS2021|[[Paper]](https://arxiv.org/abs/2105.09938)|
|Coding|DeepFix|[Deepfix: Fixing common c language errors by deep learning](https://paperswithcode.com/dataset/deepfix)|AAAI2017|[[Paper]](https://paperswithcode.com/dataset/deepfix)|
|InstructEval|InstructEval|[INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large Language Models](https://arxiv.org/pdf/2306.04757.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2306.04757.pdf) ,[[Code]](https://github.com/declare-lab/instruct-eval)|
|Role-play|RoleBench|[dataset](https://huggingface.co/datasets/ZenMoore/RoleBench/tree/main/rolebench-zh/general)|opensource|[[data]](https://huggingface.co/datasets/ZenMoore/RoleBench/tree/main/rolebench-zh/general)|
|Role-play|RoleEval|[RoleEval: A Bilingual Role Evaluation Benchmark for Large Language Models](https://arxiv.org/pdf/2312.16132.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.16132.pdf) ,[[Dataset]](https://github.com/Magnetic2014/RoleEval)|
|Role-play|CharacterEval|[CharacterEval: A Chinese Benchmark for Role-Playing Conversational Agent Evaluation](https://arxiv.org/abs/2401.01275)|Arxiv2024|[[Paper]](https://arxiv.org/abs/2401.01275) ,[[Dataset]](https://github.com/morecry/CharacterEval)|
|Hallucination|HaluEval|[HaluEval: A Large-Scale Hallucination Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2305.11747)|EMNLP2023|[[Paper]](https://arxiv.org/abs/2305.11747) ,[[Data]](https://github.com/RUCAIBox/HaluEval) ,[[HaluEval-2.0]](https://github.com/RUCAIBox/HaluEval-2.0)|
|Holistic Evaluation|helm|[Holistic Evaluation of Language Models](https://arxiv.org/abs/2211.09110)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2211.09110) ,[[Data]](https://github.com/stanford-crfm/helm)|
|Others|others|[DataSet](https://github.com/QinHsiu/Awesome-Resources/tree/main/DataSet)|2023|[[Code]](https://github.com/QinHsiu/Awesome-Resources/tree/main/DataSet)|



##### 其他资源
|Name| Description|  Resources   |
| ------- | -------  |------- |
|awesome-LLM-benchmarks|Awesome LLM Benchmarks to evaluate the LLMs across text, code, image, audio, video and more.|[[Code]](https://github.com/wgwang/awesome-LLM-benchmarks)|
|hallucination-leaderboard|Leaderboard Comparing LLM Performance at Producing Hallucinations when Summarizing Short Documents.|[[Code]](https://github.com/vectara/hallucination-leaderboard)|
