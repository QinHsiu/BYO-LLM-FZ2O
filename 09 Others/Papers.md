<!--
 * @Author: qinhsiu
 * @Email: qinhsiu@gmail.com
-->

| Title| Year |Paper|
| ------- | ----- | ------ |
|[ConvNets Match Vision Transformers at Scale](https://arxiv.org/abs/2310.16764)|CVPR2023|[[Paper]](https://arxiv.org/abs/2310.16764)|
|[Segment Anything](https://arxiv.org/abs/2304.02643)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2304.02643)|
|[Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models](https://web3.arxiv.org/abs/2304.08818)|CVPR2023|[[Paper]](https://web3.arxiv.org/abs/2304.08818)|
|[Make-A-Character: High Quality Text-to-3D Character Generation within Minutes](https://arxiv.org/pdf/2312.15430.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.15430.pdf) ,[[Code]](https://github.com/Human3DAIGC/Make-A-Character)|
|[StreamDiffusion](https://arxiv.org/pdf/2312.12491.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.12491.pdf)|
|[PixelLLM](https://arxiv.org/pdf/2312.09237.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.09237.pdf)|
|[Ferret](https://arxiv.org/pdf/2310.07704.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2310.07704.pdf)|
|[VideoPoet](https://arxiv.org/pdf/2312.14125.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.14125.pdf)|
|[STEMGEN](https://arxiv.org/pdf/2312.08723.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.08723.pdf)|
|[Amphion](https://arxiv.org/pdf/2312.09911.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.09911.pdf)|
|[Emu2](https://arxiv.org/abs/2312.13286)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2312.13286) ,[[Code]](ï¼šhttps://github.com/baaivision/Emu/tree/main/Emu2) ,[[Note]](https://mp.weixin.qq.com/s/aAS1JZWqzlxIHCe5f5MNhw)|
|[M2UGen](https://arxiv.org/pdf/2311.11255.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2311.11255.pdf) ,[[Code]](https://github.com/shansongliu/M2UGen/tree/main) ,[[Huggingface]](https://huggingface.co/M2UGen) ,[[Note]](https://mp.weixin.qq.com/s/tbn-7BA_UJC8NQ9Ht5KbLQ)|
|[Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models](https://arxiv.org/abs/2310.03059)|AAAI2024|[[Paper]](https://arxiv.org/abs/2310.03059) ,[[Code]](https://github.com/Ivan-Tang-3D/Point-PEFT) ,[[Note]](https://mp.weixin.qq.com/s/9ez2y2JicUC4DB0SiM5f6g)|
|[Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data](https://arxiv.org/pdf/2312.12832.pdf)|AAAI2024|[[Paper]](https://arxiv.org/pdf/2312.12832.pdf) ,[[Code]](https://github.com/Yiwei98/TDG) ,[[Note]](https://mp.weixin.qq.com/s/dCQL3UcF810rX1ezGDxvRg)|
|[Structure-CLIP](https://arxiv.org/pdf/2305.06152.pdf)|AAAI2024|[[Paper]](https://arxiv.org/pdf/2305.06152.pdf) ,[[Code]](https://github.com/zjukg/Structure-CLIP) ,[[Note]](https://mp.weixin.qq.com/s/F6eJfKPx_02z8x_RvXKYDA)|
|[Awesome-Multimodal-Large-Language-Models](https://arxiv.org/pdf/2306.13549.pdf)|2024|[[Code]](https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models),[[Note]](https://mp.weixin.qq.com/s/iYyLOvi8HQR3V6xAjO5q8A)|
|[PromptKD](https://arxiv.org/abs/2403.02781)|Arxiv2024|[[Code]](https://github.com/zhengli97/PromptKD),[[Note]](https://mp.weixin.qq.com/s/Eub9QELtA91bdYNkzaZfIQ)|