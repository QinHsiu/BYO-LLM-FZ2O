| Name| Type |Resources|
| ------- | ----- | ------ |
|Learn_Prompting|Prompt Lrarning|[[Code]](https://github.com/trigaten/Learn_Prompting)|
|promptbase|All things prompt engineering|[[Code]](https://github.com/microsoft/promptbase)|
|PromptPapers|Must-read papers on prompt-based tuning for pre-trained language models.|[[Code]](https://github.com/thunlp/PromptPapers)|
|OpenPrompt|An Open-Source Framework for Prompt-Learning.|[[Code]](https://github.com/thunlp/OpenPrompt)|
|Prompt-Engineering-Guide|Guides, papers, lecture, notebooks and resources for prompt engineering|[[Code]](https://github.com/dair-ai/Prompt-Engineering-Guide)|
|FastChat|An open platform for training, serving, and evaluating large language models. Release repo.|[[Code]](https://github.com/lm-sys/FastChat)|
|peft|PEFT: State-of-the-art Parameter-Efficient Fine-Tuning.|[[Code]](https://github.com/huggingface/peft)|
|LMFlow|An Extensible Toolkit for Finetuning and Inference of Large Foundation Models. Large Models for All.|[[Code]](https://github.com/OptimalScale/LMFlow)|
|ICL_PaperList|Paper List for In-context Learning|[Code](https://github.com/dqxiu/ICL_PaperList)|
|PandaGPT|PandaGPT: One Model To Instruction-Follow Them All|[[Code]](https://github.com/yxuansu/PandaGPT)|
|LLaMA-Factory|Easy-to-use LLM fine-tuning framework (LLaMA, BLOOM, Mistral, Baichuan, Qwen, ChatGLM).|[[Code]](https://github.com/hiyouga/LLaMA-Factory)|
|AgentTuning|AgentTuning: Enabling Generalized Agent Abilities for LLMs|[[Code]](https://github.com/THUDM/AgentTuning)|
|LOMO|LOMO: LOw-Memory Optimization|[[Code]](https://github.com/OpenLMLab/LOMO)|
|GPT-4-LLM|Instruction Tuning with GPT-4|[[Code]](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM)|
|PaLM-rlhf-pytorch|Implementation of RLHF (Reinforcement Learning with Human Feedback) on top of the PaLM architecture. Basically ChatGPT but with PaLM|[[Code]](https://github.com/lucidrains/PaLM-rlhf-pytorch)|


##### 其他资源

- [ChatGLM-Finetuning](https://github.com/liucongg/ChatGLM-Finetuning)
- [awesome-chatgpt-prompts-zh](https://github.com/PlexPt/awesome-chatgpt-prompts-zh)
- [ChatGPT-Prompt-Engineering-for-Developers-in-Chinese](https://github.com/GitHubDaily/ChatGPT-Prompt-Engineering-for-Developers-in-Chinese)
- [awesome-chatgpt-prompts](https://github.com/f/awesome-chatgpt-prompts)
- [大模型微调技巧](https://mp.weixin.qq.com/s/B5TE9UiuI19-XfpFcMCu1A)
- [大模型微调技巧](https://mp.weixin.qq.com/s/lYJcnUW9qtTsAF7_G8bKhA)
- [大模型微调技巧](https://mp.weixin.qq.com/s/uaIA5I4q7y0VjjHrrgJeWA)
- [LoRA加速微调](https://mp.weixin.qq.com/s/3FG5I7VPAs4Tbqpffx7tVA)
- [万字详解提示学习（Prompt Learning ）和微调 （Prompt Tuning）](https://mp.weixin.qq.com/s/-gAi4bc7pLdMnOGSS9q_zQ)
- [多模态训练，怎么对齐不同模态](https://mp.weixin.qq.com/s/1Ob7Se02nIRZybIyitMvmw) 
- [使用 DPO 微调 Llama 2](https://mp.weixin.qq.com/s/IQXO-wjWduP4Ha2deCm08g)
- [PEFT技术概述](https://mp.weixin.qq.com/s/tyVfJc_ZUZgfmNvD2FGQTw)
- [LLaMa微调实践](https://mp.weixin.qq.com/s/cgW0xMQNn7Q6N92O_4GZPw)
- [RLHF技术及其替代技术](https://mp.weixin.qq.com/s/Cf7fTJiHKDMGKo_VEj_wGA)
- [关于pt和post-t的思考](https://mp.weixin.qq.com/s/sFdnWwWXouU2ZPbf5GoXhA)
- [关于Post-training的一点思考](https://mp.weixin.qq.com/s/dvyvKExTl5t9aQDPd4hDJg)
