<!--
 * @Author: qinhsiu
 * @Email: qinhsiu@gmail.com
-->
| Title| Year |Resources|
| ------- | ----- | ------ |
|[The Ultimate Guide to Fine-Tuning LLMs from Basics to Breakthroughs: An Exhaustive Review of Technologies, Research, Best Practices, Applied Research Challenges and Opportunities](https://arxiv.org/abs/2408.13296)|Arxiv2024|[Note](https://mp.weixin.qq.com/s/aQfmOz8NnV6dXrEJ14eSGw)|
|[MoDS: Model-oriented Data Selection for Instruction Tuning](https://arxiv.org/pdf/2311.15653)|Arxiv2024|[Code](https://github.com/CASIA-LM/MoDS), [Note](https://mp.weixin.qq.com/s/aeUOoA8QL1unrB0G6mvK4g)|
|[Original Implementation of Prompt Tuning from Lester](https://aclanthology.org/2021.emnlp-main.243/)|Arxiv2021|[[Code]](https://github.com/google-research/prompt-tuning)|
|[An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks](https://arxiv.org/abs/2110.07602)|EMNLP2023|[[Paper]](https://arxiv.org/abs/2110.07602) ,[[Code]](https://github.com/THUDM/P-tuning-v2)|
|[Black-Box Tuning for Language-Model-as-a-Service](https://arxiv.org/abs/2201.03514)|ICML2022|[[Paper]](https://arxiv.org/abs/2201.03514) ,[[Code]](https://github.com/txsun1997/Black-Box-Tuning)|
|[Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2110.04366)|ICLR2022|[[Paper]](https://arxiv.org/abs/2110.04366) ,[[Code]](https://github.com/jxhe/unify-parameter-efficient-tuning)|
|[One Shot Learning as Instruction Data Prospector for Large Language Models](https://arxiv.org/abs/2312.10302)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2312.10302) ,[[Note]](https://mp.weixin.qq.com/s/YHGwdkvCN0Q1NSFsB61nYA)|
|[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288 )|Arxiv2023|[[Paper]](https://arxiv.org/abs/2307.09288 )|
|[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2106.09685) ,[[Code]](https://github.com/microsoft/LoRA)|
|[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)|NeurIPS2023|[[Paper]](https://arxiv.org/abs/2305.14314)|
|[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2305.18290)|
|[WAVECODER: WIDESPREAD AND VERSATILE ENHANCED INSTRUCTION TUNING WITH REFINED DATA GENERATION](https://arxiv.org/pdf/2312.14187.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.14187.pdf)|
|[m-LoRA: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU](https://arxiv.org/abs/2312.02515)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2312.02515) ,[[Code]](https://github.com/TUDB-Labs/multi-lora-fine-tune) ,[[Note]](https://mp.weixin.qq.com/s/8UoZy0egCui2grfU7QNm0A)|
|[Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/pdf/2401.01335.pdf)|Arxiv2024|[[Paper]](https://arxiv.org/pdf/2401.01335.pdf) ,[[Note]](https://mp.weixin.qq.com/s/aeIifHXAMULib1OpCYx7vQ)|
|[Secrets of RLHF in Large Language Models Part II: Reward Modeling](https://arxiv.org/abs/2401.06080)|Arxiv2024|[[Paper]](https://arxiv.org/abs/2401.06080) ,[[Code]](https://github.com/OpenLMLab/MOSS-RLHF) ,[[Note]](https://mp.weixin.qq.com/s/Wsuzj_k39GYzyYfwPedN4A)|
|[Self-Rewarding Language Models](https://arxiv.org/abs/2401.10020)|Arxiv2024|[[Paper]](https://arxiv.org/abs/2401.10020) ,[[Note]](https://mp.weixin.qq.com/s/tBVosNn07shQZxfvtSlaOw)|
|[Secrets of RLHF in Large Language Models Part II: Reward Modeling](https://arxiv.org/abs/2401.06080)|Arxiv2024|[[Paper]](https://arxiv.org/abs/2401.06080) ,[[Code]](https://github.com/OpenLMLab/MOSS-RLHF) ,[[Note]](https://mp.weixin.qq.com/s/IjPknZQcpMtAlCQFipqIbw)|
|[A Novel Prompt-tuning Method: Incorporating Scenario-specific Concepts into a Verbalizer](https://arxiv.org/abs/2401.05204)|Arxiv2024|[[Paper]](https://arxiv.org/abs/2401.05204) ,[[Note]](https://mp.weixin.qq.com/s/YmtXUB4HWpjM6qhKEtimbQ)|
|[PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models](https://arxiv.org/pdf/2404.02948.pdf)|Arxiv2024|[[Code]](https://github.com/GraphPKU/PiSSA),[[psft]](https://github.com/fxmeng/peft),[[Note]](https://mp.weixin.qq.com/s/EQgaHqNyNT2gvp5q4ro1xQ)|
