| Title| Year |Paper|
| ------- | ----- | ------ |
|[One Shot Learning as Instruction Data Prospector for Large Language Models](https://arxiv.org/abs/2312.10302)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2312.10302) ,[[Note]](https://mp.weixin.qq.com/s/YHGwdkvCN0Q1NSFsB61nYA)|
|[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288 )|Arxiv2023|[[Paper]](https://arxiv.org/abs/2307.09288 )|
|[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)|NeurIPS2023|[[Paper]](https://arxiv.org/abs/2305.14314)|
|[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2305.18290)|
|[WAVECODER: WIDESPREAD AND VERSATILE ENHANCED INSTRUCTION TUNING WITH REFINED DATA GENERATION](https://arxiv.org/pdf/2312.14187.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.14187.pdf)|
|[m-LoRA: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU](https://arxiv.org/abs/2312.02515)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2312.02515) ,[[Code]](https://github.com/TUDB-Labs/multi-lora-fine-tune) ,[[Note]](https://mp.weixin.qq.com/s/8UoZy0egCui2grfU7QNm0A)|
|[Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/pdf/2401.01335.pdf)|Arxiv2024|[[Paper]](https://arxiv.org/pdf/2401.01335.pdf) ,[[Note]](https://mp.weixin.qq.com/s/aeIifHXAMULib1OpCYx7vQ)|
|[Secrets of RLHF in Large Language Models Part II: Reward Modeling](https://arxiv.org/abs/2401.06080)|Arxiv2024|[[Paper]](https://arxiv.org/abs/2401.06080) ,[[Code]](https://github.com/OpenLMLab/MOSS-RLHF) ,[[Note]](https://mp.weixin.qq.com/s/Wsuzj_k39GYzyYfwPedN4A)|