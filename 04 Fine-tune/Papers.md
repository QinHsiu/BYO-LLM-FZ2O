| Title| Year |Resources|
| ------- | ----- | ------ |
|[Original Implementation of Prompt Tuning from Lester](https://aclanthology.org/2021.emnlp-main.243/)|Arxiv2021|[[Code]](https://github.com/google-research/prompt-tuning)|
|[An optimized deep prompt tuning strategy comparable to fine-tuning across scales and tasks](https://arxiv.org/abs/2110.07602)|EMNLP2023|[[Paper]](https://arxiv.org/abs/2110.07602) ,[[Code]](https://github.com/THUDM/P-tuning-v2)|
|[Black-Box Tuning for Language-Model-as-a-Service](https://arxiv.org/abs/2201.03514)|ICML2022|[[Paper]](https://arxiv.org/abs/2201.03514) ,[[Code]](https://github.com/txsun1997/Black-Box-Tuning)|
|[Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2110.04366)|ICLR2022|[[Paper]](https://arxiv.org/abs/2110.04366) ,[[Code]](https://github.com/jxhe/unify-parameter-efficient-tuning)|
|[One Shot Learning as Instruction Data Prospector for Large Language Models](https://arxiv.org/abs/2312.10302)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2312.10302) ,[[Note]](https://mp.weixin.qq.com/s/YHGwdkvCN0Q1NSFsB61nYA)|
|[Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/abs/2307.09288 )|Arxiv2023|[[Paper]](https://arxiv.org/abs/2307.09288 )|
|[LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2106.09685) ,[[Code]](https://github.com/microsoft/LoRA)|
|[QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)|NeurIPS2023|[[Paper]](https://arxiv.org/abs/2305.14314)|
|[Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/abs/2305.18290)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2305.18290)|
|[WAVECODER: WIDESPREAD AND VERSATILE ENHANCED INSTRUCTION TUNING WITH REFINED DATA GENERATION](https://arxiv.org/pdf/2312.14187.pdf)|Arxiv2023|[[Paper]](https://arxiv.org/pdf/2312.14187.pdf)|
|[m-LoRA: High-Throughput LoRA Fine-Tuning of Large Language Models with a Single GPU](https://arxiv.org/abs/2312.02515)|Arxiv2023|[[Paper]](https://arxiv.org/abs/2312.02515) ,[[Code]](https://github.com/TUDB-Labs/multi-lora-fine-tune) ,[[Note]](https://mp.weixin.qq.com/s/8UoZy0egCui2grfU7QNm0A)|
|[Self-Play Fine-Tuning Converts Weak Language Models to Strong Language Models](https://arxiv.org/pdf/2401.01335.pdf)|Arxiv2024|[[Paper]](https://arxiv.org/pdf/2401.01335.pdf) ,[[Note]](https://mp.weixin.qq.com/s/aeIifHXAMULib1OpCYx7vQ)|
|[Secrets of RLHF in Large Language Models Part II: Reward Modeling](https://arxiv.org/abs/2401.06080)|Arxiv2024|[[Paper]](https://arxiv.org/abs/2401.06080) ,[[Code]](https://github.com/OpenLMLab/MOSS-RLHF) ,[[Note]](https://mp.weixin.qq.com/s/Wsuzj_k39GYzyYfwPedN4A)|
